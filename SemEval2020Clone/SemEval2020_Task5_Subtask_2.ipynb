{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlTujgsUAGFA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-2/train.csv /content\n",
        "!cp /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-2/test.csv /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yB4EnsYmpn5-"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>203551</td>\n",
              "      <td>Sandwich in Kent; until 2011 the flagship of P...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>203552</td>\n",
              "      <td>If the deals were properly accounted for, Bank...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>203553</td>\n",
              "      <td>These distortions have seen one party or the o...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>203554</td>\n",
              "      <td>If the Clinton scandal had included storylines...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>203555</td>\n",
              "      <td>\"If there was a better way to say it's the hig...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id                                               text labels\n",
              "1  203551  Sandwich in Kent; until 2011 the flagship of P...       \n",
              "2  203552  If the deals were properly accounted for, Bank...       \n",
              "3  203553  These distortions have seen one party or the o...       \n",
              "4  203554  If the Clinton scandal had included storylines...       \n",
              "5  203555  \"If there was a better way to say it's the hig...       "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "prefix = '/content/'\n",
        "test_df = pd.read_csv(prefix + 'test.csv', header=None) # re-name test.csv \n",
        "test_df=test_df.drop(index=0)\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    'id':test_df[0],\n",
        "    'text': test_df[1],\n",
        "    'labels': ''\n",
        "})\n",
        "\n",
        "# display \n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4DctzKMLVOx"
      },
      "outputs": [],
      "source": [
        "!pip install allennlp allennlp-models\n",
        "\n",
        "!pip install nltk\n",
        "!pip install tqdm\n",
        "!pip install spacy \n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4IkB2ctejwTR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (5.2.0) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using AllenNLP requires the python packages Spacy, Pytorch and Numpy to be installed. Please see https://github.com/allenai/allennlp for installation instructions.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'spacy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/home/cytech/ing3/recherche/implementation/SemEval2020Clone/SemEval2020_Task5_Subtask_2.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cytech/ing3/recherche/implementation/SemEval2020Clone/SemEval2020_Task5_Subtask_2.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mallennlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpredictors\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpredictor\u001b[39;00m \u001b[39mimport\u001b[39;00m Predictor\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cytech/ing3/recherche/implementation/SemEval2020Clone/SemEval2020_Task5_Subtask_2.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mallennlp_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstructured_prediction\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cytech/ing3/recherche/implementation/SemEval2020Clone/SemEval2020_Task5_Subtask_2.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m predictor \u001b[39m=\u001b[39m Predictor\u001b[39m.\u001b[39mfrom_path(\u001b[39m\"\u001b[39m\u001b[39mhttps://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/allennlp/__init__.py:11\u001b[0m\n\u001b[1;32m      6\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumpy.ufunc size changed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[39m# On some systems this prevents the dreaded\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39m# ImportError: dlopen: cannot load any more object with static TLS\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mspacy\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mtorch\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mnumpy\u001b[39;00m  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m     15\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing AllenNLP requires the python packages Spacy, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPytorch and Numpy to be installed. Please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/allenai/allennlp for installation instructions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m     )\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
          ]
        }
      ],
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.structured_prediction\n",
        "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CZ-J9aocvFD"
      },
      "outputs": [],
      "source": [
        "prediction = predictor.predict(\n",
        "        sentence = \"'Age may have dimmed their employment prospects, but older people often had the financial firepower to start up their own businesses - though should they have chosen to embark upon a second career, it would have been good doing something that they always wanted to do.'\"\n",
        ")\n",
        "#prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIJgOTAePV-e"
      },
      "source": [
        "START AGAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7r33OAFtOM9"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def extract_modal(obj):\n",
        "    tags = list() \n",
        "    parts = list()\n",
        "    words = list()\n",
        "    nouns = list()\n",
        "    verbs = list()\n",
        "\n",
        "    includes= list()\n",
        "    cutouts = list()\n",
        "\n",
        "    conseqs = list()\n",
        "    anteced = list()\n",
        "\n",
        "    def extract(obj):\n",
        "\n",
        "      if isinstance(obj, dict):\n",
        "        for k, v in obj.items():\n",
        "          if k=='nodeType':\n",
        "            tags.append(v)\n",
        "          if k=='word':\n",
        "            words.append(v)\n",
        "          #if k=='children': # as below\n",
        "          if isinstance(v, (dict, list)):\n",
        "            extract(v)\n",
        "        \n",
        "      elif isinstance(obj, list):\n",
        "        for item in obj:\n",
        "          if isinstance(item, dict):\n",
        "            extract(item)\n",
        "\n",
        "            if len(words)>0:\n",
        "              word = item['word'].rstrip()\n",
        "              zoom = 0\n",
        "              index=-1\n",
        "              if 'SBAR' in tags: \n",
        "                index = max(idx for idx, val in enumerate(tags) if val=='SBAR')\n",
        "                zoom = index\n",
        "              if 'S' in tags:\n",
        "                index = max(idx for idx, val in enumerate(tags) if val=='S')\n",
        "                if index > zoom:\n",
        "                  zoom = index\n",
        "              head = words[zoom]\n",
        "              #print(tags, zoom, head)\n",
        "              if (\n",
        "                  'would'==word.lower() or 'wouldn'==word.lower() or 'wouldn\\'t'==word.lower() or 'woulnd\\'t'==word.lower() or \n",
        "                  'could'==word.lower() or 'could\\'t'==word.lower() or 'coulnd\\'t'==word.lower() or #'can'==word.lower() or\n",
        "                  'might'==word.lower() or 'mightn\\'t'==word.lower() or 'mighn\\'t'==word.lower()  or 'may'==word.lower() or\n",
        "                  #'should'==word.lower() or 'shouldn\\'t'==word.lower() or 'shoulnd\\'t'==word.lower() or \n",
        "                  'd'==word.lower() or word.lower().rstrip().endswith('\\'d') or 'ought'==word.lower() or\n",
        "                  'will have been' in word.lower() or 'would\\'ve' in word.lower() or 'wouldn\\'ve' in word.lower()\n",
        "              ):\n",
        "                phrase = scrap_from(obj,item)\n",
        "                #print(phrase,'|',head)\n",
        "                if phrase not in head or len(word_tokenize(head)) < 2:\n",
        "                  head = phrase \n",
        "                parts.append({'tag':'MD','word':head,'text':obj})\n",
        "                includes.append(word)\n",
        "      \n",
        "              #if 'MD' in tags:    \n",
        "              #  if word not in string.punctuation:  \n",
        "              #    parts.append({'tag':'MD','word':word,'text':obj})\n",
        "              #    includes.append(word)\n",
        "\n",
        "              if 'VBP' in tags: # 'MD' misspelled (and hence mistagged) as 'VBP'\n",
        "                if (\n",
        "                    word.lower().startswith('wouln') or word.lower().startswith('couln') or word.lower().startswith('shouln')\n",
        "                ):     \n",
        "                  parts.append({'tag':'MD','word':head,'text':obj}) # treat 'VBP' as 'MD'\n",
        "                  includes.append(word)\n",
        "\n",
        "              if 'NP' in tags: # for debugging only!\n",
        "                #print('NP:', word)\n",
        "                #print('head:',head)\n",
        "                nouns.append({'tag':'NP','word':word,'text':obj})\n",
        "\n",
        "              #if 'SINV' in tags: # Inverted declarative sentence (eg Had it been...)\n",
        "              #  parts.append({'tag':'SINV','word':head,'text':obj}) \n",
        "\n",
        "              if 'VP' in tags:\n",
        "                verbs.append({'tag':'VP','word':word,'text':obj})\n",
        "                if 'wish' in word.lower() or 'shoul' in word.lower(): # \"in\"(not \"==\")\n",
        "                  #print(word,' | ', head)\n",
        "                  if head in word or len(word_tokenize(head)) < 2:\n",
        "                    head = word\n",
        "                  parts.append({'tag':'VP','word':head,'text':obj})\n",
        "                  includes.append(word)\n",
        "\n",
        "              if 'VBD' in tags: \n",
        "                if 'were'==word.lower() or 'was'==word.lower() or 'had'==word.lower():\n",
        "                  if word not in head or len(word_tokenize(head)) < 2:\n",
        "                    head = scrap_from(obj,item)\n",
        "                  parts.append({'tag':'VBD','word':head,'text':obj})\n",
        "                  includes.append(word)\n",
        "                \n",
        "              if 'VBN' in tags: # parser mistakes 'VBN' for 'VBD'(past) \n",
        "                if 'had'==word.lower():\n",
        "                  if word not in head or len(word_tokenize(head)) < 2:\n",
        "                    head = scrap_from(obj,item)\n",
        "                  parts.append({'tag':'VBN','word':head,'text':obj})\n",
        "                  includes.append(word) \n",
        "\n",
        "              if 'VBG' in tags:  \n",
        "                if 'having'==word.lower():\n",
        "                  if word not in head or len(word_tokenize(head))< 2:\n",
        "                    head = scrap_from(obj,item)\n",
        "                  parts.append({'tag':'VBG','word':head,'text':obj})\n",
        "                  includes.append(word)\n",
        "\n",
        "\n",
        "              if 'RB' in tags: \n",
        "                if 'even'==word.lower():\n",
        "                  #if word not in head:\n",
        "                  #  head = scrap_from(obj,item)\n",
        "                  parts.append({'tag':'RB','word':head,'text':obj}) # treat 'RB' as 'MD'\n",
        "                  includes.append(word)\n",
        "                      \n",
        "              if 'IN' in tags: \n",
        "                if (\n",
        "                    'if' == word.lower() or 'unless'== word.lower() or # 'under', 'given' ??\n",
        "                    'with'==word.lower() or 'without'==word.lower()\n",
        "                ): \n",
        "                  #if word not in head:\n",
        "                  head = scrap_from(obj,item)\n",
        "                  parts.append({'tag':'IN','word':head,'text':obj})\n",
        "                  includes.append(word)     \n",
        "\n",
        "              if 'CC' in tags:\n",
        "                if (\n",
        "                    'but' == word.lower()\n",
        "                ): \n",
        "                  phrase = scrap_from(obj,item)\n",
        "                  if len(tags) > 1:   #   unless first\n",
        "                    phrase = phrase[4:]# 3+1=leading 'but '\n",
        "                  #print(phrase, head)  \n",
        "                  if len(phrase)>0 and phrase not in head:\n",
        "                    head = phrase\n",
        "                  parts.append({'tag':'CC','word':head,'text':obj})\n",
        "                  includes.append(word)\n",
        "\n",
        "              tag = tags[-1]\n",
        "              phrase = words[-1]\n",
        "              if 'S'==tag or 'CC'==tag:\n",
        "                cut = True\n",
        "                for include in includes:\n",
        "                  if include in phrase:\n",
        "                    cut = False\n",
        "                if cut and phrase not in cutouts:\n",
        "                  cutouts.append(phrase)\n",
        "        return\n",
        "\n",
        "    extract(obj)\n",
        " \n",
        "    #print(cutouts)\n",
        "   \n",
        "    anteced,conseqs = combine(parts,anteced,conseqs)\n",
        "    anteced,conseqs = combine(parts,anteced,conseqs)\n",
        "\n",
        "    return conseqs, anteced, nouns, verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s-TFTZL6Mok"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def separate_mod(md):\n",
        "  phrases = [md]  \n",
        " \n",
        "  for phrase in phrases:\n",
        "    if 'if ' in phrase.lower(): # 'would ask if I did something wrong\"\n",
        "      phrase = phrase[0:phrase.lower().index('if ')]\n",
        "    if ' wish ' in phrase.lower(): # 'wish I could say I wouldn't do it\"\n",
        "      phrase = phrase[0:phrase.lower().index(' wish ')]\n",
        "    if 'even ' in phrase.lower(): # 'even with a lot of money..\",\"even if\"\n",
        "      phrase = phrase[0:phrase.lower().index('even ')]    \n",
        "    #if ' with' in phrase.lower(): # 'without knowing if I can count on..\"\n",
        "    #  phrase = phrase[0:phrase..lower().index(' with')]\n",
        "    \n",
        "    if 'will have been' in phrase.lower() or 'would\\'ve' in phrase.lower() or 'wouldn\\'ve' in phrase.lower():\n",
        "      print('phrase:',phrase)\n",
        "      return phrase\n",
        "\n",
        "    words = word_tokenize(phrase)\n",
        "    for word in words:\n",
        "      if (\n",
        "        'would'==word.lower() or 'wouldn'==word.lower() or 'wouldn\\'t'==word.lower() or 'woulnd\\'t'==word.lower() or \n",
        "        'could'==word.lower() or 'could\\'t'==word.lower() or 'coulnd\\'t'==word.lower() or #'can'==word.lower() or\n",
        "        'might'==word.lower() or 'mightn\\'t'==word.lower() or 'mighn\\'t'==word.lower()  or 'may'==word.lower() or\n",
        "        #'should'==word.lower() or 'shouldn\\'t'==word.lower() or 'shoulnd\\'t'==word.lower() or \n",
        "        'd'==word.lower() or word.lower().rstrip().endswith('\\'d') or 'ought'==word.lower()\n",
        "      ):\n",
        "        return phrase[phrase.lower().index(word.lower()):]\n",
        "  return ''     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDJ7trrg3yzq"
      },
      "outputs": [],
      "source": [
        "def combine(parts,anteced,conseqs):\n",
        " \n",
        "  for part in parts:\n",
        "      tag = part['tag']\n",
        "      clause= part['word']    \n",
        "      tockenized = word_tokenize(clause)\n",
        "      tokens = set(tockenized)\n",
        "\n",
        "      if part['tag'] == 'MD':\n",
        "        exclude = False\n",
        "        for antecedent in sorted(anteced,key=len):\n",
        "          if antecedent in clause: \n",
        "            exclude = True\n",
        "            break\n",
        "        for consequent in reversed(sorted(conseqs,key=len)):\n",
        "          over = separate_from(consequent, clause)\n",
        "          if over != consequent:\n",
        "            exclude = True\n",
        "            break\n",
        "        if not exclude and clause not in conseqs: \n",
        "          conseqs.append(clause)\n",
        "        #print(tag,':',clause,' [out of] ', len(parts),'-exclude?-',exclude)  \n",
        "      \n",
        "      if len(tokens) > 1: # eg. 'But', 'Nonetheless'\n",
        "        if part['tag'] in 'SINV VBD VBN VP IN RB CC':\n",
        "          exclude = False \n",
        "          for antecedent in reversed(sorted(anteced,key=len)):\n",
        "            over = separate_from(antecedent,clause)\n",
        "            if over != antecedent:\n",
        "              if over.lower().startswith('if ') or over.lower().startswith('i wish ') : # allows \"Had I have my wish\"\n",
        "                exclude = True\n",
        "                break    \n",
        "          if not exclude and clause not in anteced:\n",
        "            anteced.append(clause)\n",
        "          #print(tag,':',clause,' [out of] ',len(parts),'-exclude?-',exclude)  \n",
        "\n",
        "  return anteced, conseqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MeCoOLXx8IU"
      },
      "outputs": [],
      "source": [
        "from nltk import sent_tokenize\n",
        "\n",
        "def scrap_from(obj, item=None): \n",
        "  word = str() \n",
        "  prev = str() \n",
        "  for head in obj: # head word\n",
        "    if item == None or head['word'] == item['word']: \n",
        "      word = ' '# start from specified item fragment \n",
        "    if len(word) > 0: \n",
        "      word = word.strip() \n",
        "      if (not (  \n",
        "              word.lower()=='\\'m' or word.lower()=='\\'ll' or\n",
        "              word.lower()=='\\'s' or word.lower()=='d' or # I'd be...\n",
        "              word.lower()=='\\'re' or word.lower()=='\\'ve' or word.lower()=='n\\'t') and  \n",
        "          not (prev.lower().endswith('s') and word.lower()=='\\' ') and # before\\':peoples'\n",
        "          not (word.lower() in '+.: m k x' and prev[-1:].isnumeric()) and # eg:'10:30am', '3M'\n",
        "          not (prev.lower() in '+' and word.isnumeric()) and\n",
        "          not (\n",
        "              prev.lower() in '¥$€£&@({[/---#'  or  # after(eg 'S& P')\n",
        "              word.lower() in '^~*:&%)}]/---...,;' # before(eg '% 5 M')\n",
        "          )\n",
        "      ): \n",
        "        word = word + ' ' # add space, except the aformentioned cases\n",
        "      next = sent_tokenize(head['word'])[0]\n",
        "      word = word + next  # add next sentence\n",
        "      prev = word                 \n",
        "  word = normalize(word)     \n",
        "  #print(word)    \n",
        "  return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm_UQ7ejvUsN"
      },
      "outputs": [],
      "source": [
        "def separate_children(obj):\n",
        "    arr = list()\n",
        "\n",
        "    def separate(obj, arr):\n",
        "        \n",
        "        if isinstance(obj, dict):\n",
        "\n",
        "            for k, v in obj.items():\n",
        "              if k != 'children':\n",
        "                if isinstance(v, (dict, list)):\n",
        "                    separate(v, arr)\n",
        "                elif k == 'word':      \n",
        "                    arr.append(v)\n",
        "\n",
        "        elif isinstance(obj, list):\n",
        "            for item in obj:\n",
        "                separate(item, arr)\n",
        "         \n",
        "        return arr\n",
        "\n",
        "    results = separate(obj, arr)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XYFHp-5KyQ7"
      },
      "outputs": [],
      "source": [
        "def extract_constituents(prediction):\n",
        " for k, v in prediction.items():\n",
        "  if k=='hierplane_tree':\n",
        "    for ke, val in v.items():\n",
        "      if ke=='root':\n",
        "        for key, value in val.items():\n",
        "          if key == 'children':\n",
        "            return extract_modal(value)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pURMbvkG2iTY"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def simplify(constit):\n",
        "  if isinstance(constit, str):\n",
        "    word = normalize(constit)\n",
        "    return word\n",
        "   \n",
        "  constit = constit['text']\n",
        "\n",
        "  gist = list();\n",
        "  unique = set() \n",
        "  for c in constit:\n",
        "      p = separate_children(c) # list of c['word']s\n",
        "      for head in sorted(p,key=len):\n",
        "        #print(head) # head phrase (few words or one char ',')\n",
        "        tokenized = word_tokenize(head)\n",
        "        tokens = set(tokenized) #  without duplicates (eg ',')\n",
        "        if len(unique.intersection(tokens)) <= len(tokens):\n",
        "          unique.update(tokens) # without duplicate tokens!!\n",
        "          gist.extend(tokenized)\n",
        "  #print(gist)     \n",
        "  result = str()\n",
        "  prev = str()\n",
        "  for word in gist:\n",
        "    word = word.strip()\n",
        "    if (\n",
        "        not (\n",
        "             word.lower()=='\\'m' or word.lower()=='\\'ll' or \n",
        "             word.lower()=='\\'s' or word.lower()=='d' or # I'd be doing...\n",
        "             word.lower()=='\\'re' or word.lower()=='\\'ve' or word=='n\\'t'\n",
        "        ) and\n",
        "        not (prev.lower().endswith('s') and word.lower()=='\\' ') and # before\\':peoples'\n",
        "        not (word.lower() in '+.: m k x' and prev[-1:].isnumeric()) and # after '10:30am'\n",
        "        not (prev.lower() in '+' and word.isnumeric()) and\n",
        "        not (prev.lower()=='i' and word.lower()=='d') and # ID - identification\n",
        "        not  \n",
        "        (\n",
        "          prev.lower() in '¥$€£&@({[/---#' or  # ' ' after\n",
        "          word.lower() in '^~*:&%)}]/---...,;'# or before\n",
        "        )\n",
        "    ): \n",
        "      result = result + ' '\n",
        "      #print(prev,'-->',word)\n",
        "    prev = word\n",
        "    result = result + word \n",
        "  result = normalize(result)   \n",
        "  #print(phrase,':',result)  \n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c45HuHKFl-Ax"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "regex = '\\\\\\'\\s+([^\\']+)\\s+\\\\\\''\n",
        "def compact(fragment):\n",
        "# specify the number of replacements by changing the 4th argument\n",
        "  result = re.sub(regex, '\\''+r'\\1'+'\\'' , fragment)\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CoPtjXKBtrj"
      },
      "outputs": [],
      "source": [
        "def normalize(fragment):\n",
        "  r = compact(fragment).strip()\n",
        "  r = r.replace('   ',' ') \n",
        "  r = r.replace('  ',' ')\n",
        "  r = r.replace(' ,',',') \n",
        "  r = r.replace(' .','.')\n",
        "  r = r.replace('Mr.','Mr. ')\n",
        "  r = r.replace('Ms.','Ms. ')\n",
        "  r = r.replace('Mrs.','Mrs. ')\n",
        "  r = r.replace('.  ','. ') # Mr./Ms./Mrs.\n",
        "  r = r.replace('... ','...')\n",
        "  r = r.replace(' --- ','---')\n",
        "  r = r.replace(' -- ','--')\n",
        "  r = r.replace(' - ','-')\n",
        "  r = r.replace(' !','!')\n",
        "  r = r.replace(' ?','?')\n",
        "  r = r.replace('$ ','$')\n",
        "  r = r.replace('# ','#')\n",
        "  r = r.replace(' %','%')\n",
        "  r = r.replace('( ','(')\n",
        "  r = r.replace(' )',')')\n",
        "  r = r.replace(' ;',';')\n",
        "  r = r.replace(' :',':') \n",
        "  r = r.replace(': ',':') #50/50 - carefull!\n",
        "  r = r.replace(' nt','nt')\n",
        "  r = r.replace(' (k)','(k)') #401(k)\n",
        "  r = r.replace(' (tm)','(tm)')\n",
        "  r = r.replace('n na ','nna ')\n",
        "  r = r.replace(' \\'m ','\\'m ')\n",
        "  r = r.replace('s \\' ','s\\' ') # was 'neurtralized'?\n",
        "  r = r.replace(' \\'s ','\\'s ') # a 'surgical' strike!\n",
        "  r = r.replace(' \\'d ','\\'d ')\n",
        "  r = r.replace(' \\'re ','\\'re ')\n",
        "  r = r.replace(' \\'ve ','\\'ve ')\n",
        "  r = r.replace(' \\'ll ','\\'ll ')\n",
        "  r = r.replace(' n\\'t','n\\'t') # what about UPPER()?\n",
        "  r = r.replace(' \\' ',' \\'')\n",
        "  r = r.rstrip()\n",
        "  return r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUKIAsKb9Cwf"
      },
      "outputs": [],
      "source": [
        "def defragment(fragments, text, original, removed):\n",
        "  result = []\n",
        "  candid = []\n",
        "  defrag = []\n",
        "  prev = str() # previous fragment\n",
        "  end = 0 # end of previous fragment  \n",
        "  for f in sorted(fragments,key=len):\n",
        "    start= 0 # start of next fragment\n",
        "    r = normalize(f)\n",
        "    #print(prev+' | '+r)\n",
        "    try: \n",
        "      start = text.index(r)\n",
        "    except ValueError:\n",
        "      #print('\\n{'+r+'} not in:\\n '+text) # this is now in offset()\n",
        "      #continue\n",
        "      pass\n",
        "    if start > end: # assume no overlap with prev (ie separte fragment) \n",
        "      end = start + len(r)\n",
        "      prev = r\n",
        "      if end <= len(text):\n",
        "        candid.append(overlay(start, {'labels':0, 'text':r, 'start':start, 'end':end}, original, removed))  \n",
        "    elif start <= end: # next fragment is located before prev end\n",
        "      if start >= 0:\n",
        "        end = start + len(r)\n",
        "        prev = r\n",
        "        if end <= len(text):\n",
        "          candid.append(overlay(start, {'labels':0, 'text':r, 'start':start, 'end':end}, original, removed))\n",
        "  \n",
        "  if len(candid) == 1:\n",
        "    candid[0].update(labels = 1) # gold  \n",
        "\n",
        "  result.extend(candid)\n",
        "\n",
        "  return result       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDbpK2N_Md_0"
      },
      "outputs": [],
      "source": [
        "def overlay(starting, fragment, original, removed):\n",
        "  assert isinstance(fragment, dict)\n",
        "\n",
        "  base = offset(fragment['text'], starting, original)\n",
        "  text = base[0]\n",
        "  start= base[1]\n",
        "  end = base[2]\n",
        "\n",
        "  for r in removed:\n",
        "    #print(r[0],start,r[1],end)\n",
        "    if r[0] <= start:\n",
        "      start=start + 1\n",
        "      #print(start,text,end)\n",
        "    if r[0] <= end:\n",
        "      end = end + 1\n",
        "      #print(start,text,end) \n",
        "    \n",
        "  fragment.update(text = text)  \n",
        "  fragment.update(start=start) \n",
        "  fragment.update(end = end) \n",
        "\n",
        "  return fragment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt4ghi_XxGa-"
      },
      "outputs": [],
      "source": [
        "def offset(text, starting, original):\n",
        "  #print(starting, text)\n",
        "  start=0\n",
        "  end = 0\n",
        "  prev = str()\n",
        "  for c in text:\n",
        "    try: \n",
        "      start = original.index(prev + c, starting)\n",
        "      end = start + len(prev) + len(c) - 1\n",
        "      #print(c, start, prev, end)\n",
        "    except ValueError:\n",
        "      #print(c, start, prev, end)\n",
        "      if c==' ': # eg.'Pfizer (NYSE: '+'PFE)'\n",
        "        last = original.find(prev, starting)\n",
        "        if last > -1: # back 1 space helped\n",
        "          prev = prev # + c\n",
        "          continue    # skip 1 space\n",
        "        else:   \n",
        "          if end >= 0: # reverse from end\n",
        "            start = end - len(prev) + 1\n",
        "            text = original[start:end+1]\n",
        "            #print(start, prev, end, text)\n",
        "          else:\n",
        "            print('\\n\\\"'+prev+'\\\" not from '+str(starting) + ' in:'+original)\n",
        "          return text, start, end  \n",
        "      else: \n",
        "        last = original.find(prev+' '+c, starting)\n",
        "        if last > -1:                  # text==\"had I had advice... 20+ years ago\"\n",
        "          prev = prev + ' ' + c        # prev==\"had I \"\n",
        "          continue\n",
        "        else:                          \n",
        "          last = original.find(prev+c) # with space\n",
        "          if last == -1:       \n",
        "            last = original.find(prev) # w/o space\n",
        "          if last > 0:                \n",
        "            start = last-1\n",
        "            end = start+len(prev)+len(c) \n",
        "            prev = prev+c\n",
        "            #print(c,start,prev,end,text)\n",
        "            continue\n",
        "          if start >= 0: # reverse from start\n",
        "            end = start + len(prev) + len(c)\n",
        "            text = original[start:end]\n",
        "            #print(c,start,prev,end,text)\n",
        "          else:\n",
        "            print('\\n\\\"'+prev+c+'\\\" not from '+str(starting) + ' in:'+original)  \n",
        "          return text,start,end \n",
        "\n",
        "    prev = prev + c\n",
        "\n",
        "  #print(start, prev, end, text)  \n",
        "  return text, start, end  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msFC5TstNCsu"
      },
      "outputs": [],
      "source": [
        "def record(original):\n",
        "  base=[(i, c) for i, c in enumerate(original)]\n",
        "  return base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-pu1f8IN7VA"
      },
      "outputs": [],
      "source": [
        "def contract(text, base):\n",
        "  removed = []\n",
        "  open = False\n",
        "  for i in range(len(base)-1):\n",
        "    sequence = base[i][1]\n",
        "    if i < len(base) - 1:\n",
        "      sequence = sequence + base[i+1][1]\n",
        "    if sequence==' \\\"' or sequence.startswith('\\\"'):\n",
        "      open = True                 \n",
        "    elif sequence=='\\\" ' or sequence.endswith('\\\"'):\n",
        "      open = False    \n",
        "    if text.find(sequence,base[i][0]-i)==-1:\n",
        "      if open and sequence != ' \\\"':\n",
        "        removed.append(base[i])\n",
        "  return removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT0KNkJn_Tuj"
      },
      "outputs": [],
      "source": [
        "def separate_from(md, pa):\n",
        "  index = -1\n",
        "  if pa in md and len(md) > len(pa):\n",
        "    index = md.find(pa)\n",
        "    if index > 0 :\n",
        "      md = md[0:index]\n",
        "      #print('|'+md+'{...}')\n",
        "    elif index == 0:\n",
        "      md = md.replace(pa,'')\n",
        "      #print('|{...}'+md)  \n",
        "  return md     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ravlAydJ3fsR"
      },
      "outputs": [],
      "source": [
        "def separate_right(md, pa): \n",
        "  suffix = str()\n",
        "  index = -1\n",
        "  if pa in md and len(pa) < len(md):\n",
        "    index = md.rfind(pa)    # rfind for highest index \n",
        "    #print(len(pa),index,len(md))\n",
        "    if index + len(pa) < len(md):\n",
        "      suffix = md[index + len(pa):]  \n",
        "  return suffix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWcRlR-p34-q"
      },
      "outputs": [],
      "source": [
        "def separate_left(md, pa):\n",
        "  prefix = str()\n",
        "  index = -1\n",
        "  if pa in md and len(md) > len(pa):\n",
        "    index = md.find(pa)\n",
        "    if index > 0 :\n",
        "      prefix = md[0:index]\n",
        "    elif index == 0:\n",
        "      prefix = md.replace(pa,'') # trim pa from left\n",
        "  return prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeVisyM8bjig"
      },
      "outputs": [],
      "source": [
        "def separate(modal,past):\n",
        "  for md in sorted(modal,key=len):\n",
        "    for pa in reversed(sorted(past,key=len)):\n",
        "      #print(md,' | ',pa)\n",
        "#'For example, people diagnosed with colon cancer might typically live beyond a decade, compared to just seven months...\n",
        "                # | if they had been diagnosed 40 years ago.'\n",
        "\n",
        "      if pa in md:\n",
        "        if md.find(pa)>0:\n",
        "          new = md[0:md.index(pa)]\n",
        "        else:\n",
        "          new = md.replace(pa,'')\n",
        "        new = new.strip()\n",
        "      \n",
        "        if len(new)==0: # md == pa\n",
        "          continue\n",
        "          #new = separate_mod(md) # 'might have lived beyond a decade'\n",
        "\n",
        "        if len(new)>0:\n",
        "          if new == md: # separate_mod(md): empty  \n",
        "            if md in modal:\n",
        "              modal.remove(md)\n",
        "              print('remove [C]:',md,'[b/c: A={C}]')\n",
        "            continue  \n",
        "\n",
        "          #print(new)\n",
        "          if len(word_tokenize(new)) > 1 and not (\n",
        "            new.lower().lstrip().startswith('if ') or\n",
        "            new.lower().lstrip().startswith('wish ')\n",
        "          ):\n",
        "            if md in modal:\n",
        "              modal.remove(md)  \n",
        "              print('replace[C]:',md,'[with:]',new)\n",
        "            if new not in modal:\n",
        "              modal.append(new)\n",
        "              print('append [C]:',new)\n",
        "            if md in past:\n",
        "              past.remove(md)\n",
        "              print('remove [A]:',md,'[C]:',new) \n",
        "\n",
        "            md = new  \n",
        "\n",
        "      if md in pa:\n",
        "        if pa.find(md)>0:\n",
        "          new = pa[0:pa.index(md)]\n",
        "        else:\n",
        "          new = pa.replace(md,'')  \n",
        "        new = new.strip()    \n",
        "\n",
        "        if len(new)==0: # md == pa\n",
        "          if pa in past:\n",
        "            past.remove(pa)\n",
        "            print('remove [A]:',pa,'[C]:',md)\n",
        "\n",
        "        if len(new) > 0: \n",
        "          if new.startswith(', '):\n",
        "            new = new[2:]\n",
        "        \n",
        "        if len(new)>0: # md is pa or part of pa\n",
        "          if len(word_tokenize(new)) > 1 and not (\n",
        "            new.lower().lstrip().startswith('if ') or # is pa\n",
        "            new.lower().lstrip().startswith('wish ')\n",
        "          ):\n",
        "            if pa in past:\n",
        "              past.remove(pa)\n",
        "              print('remove [A]:',pa,'[C]:',md)\n",
        "            if pa not in modal: # pa is not md and md is not pa\n",
        "              new = separate_mod(pa) # separate md part from pa\n",
        "              if len(word_tokenize(new)) > 1:\n",
        "                if new not in modal:  \n",
        "                  modal.append(new)\n",
        "                  print('replace[C]:',md,'[with:]',new) \n",
        "\n",
        "                prefix = separate_left(pa,new)\n",
        "                suffix = ', ' # default\n",
        "                last = prefix.rfind(suffix)\n",
        "                if last > 0:\n",
        "                  suffix = prefix[last:]\n",
        "                  prefix = prefix[0:last]\n",
        "                  if len(word_tokenize(prefix)) > 1:\n",
        "                    if prefix not in past:\n",
        "                      past.append(prefix)\n",
        "                      print('append [A]:',prefix)\n",
        "\n",
        "            continue\n",
        "\n",
        "          if new != pa and len(word_tokenize(new)) > 1 and (',' in new or not (\n",
        "            new.lower().rstrip().endswith(' wish') or # \"... you wish\"\n",
        "            new.lower().rstrip().endswith(' and') or \n",
        "            new.lower().rstrip().endswith(' that') or\n",
        "            new.lower().rstrip().endswith(' this') or\n",
        "            new.lower().rstrip().endswith(' these') or\n",
        "            new.lower().rstrip().endswith(' those') or\n",
        "            new.lower().rstrip().endswith(' what') or\n",
        "            new.lower().rstrip().endswith(' when') or\n",
        "            new.lower().rstrip().endswith(' whom') or\n",
        "            new.lower().rstrip().endswith(' who') or\n",
        "            new.lower().rstrip().endswith(' why')\n",
        "          )):\n",
        "            if (new.lower().rstrip().endswith(', that') or\n",
        "                new.lower().rstrip().endswith(', this') or\n",
        "                new.lower().rstrip().endswith(', these') or\n",
        "                new.lower().rstrip().endswith(', those')\n",
        "            ):\n",
        "              mark = new.rfind(',')\n",
        "              if mark > -1:\n",
        "                new = new[0:mark]\n",
        "              mark = new.rfind(';')\n",
        "              if mark > -1:\n",
        "                new = new[0:mark]           \n",
        "            if pa in past: \n",
        "              past.remove(pa)\n",
        "              print('remove [A]:',pa)\n",
        "            if new not in past:\n",
        "              print('replace[A]:',pa,'[with:]',new)\n",
        "              past.append(new)          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRDL4wXADzK8"
      },
      "outputs": [],
      "source": [
        "def deoverlap(past):\n",
        "  for pal in sorted(past,key=len): # long/short\n",
        "    for pas in reversed(sorted(past,key=len)):\n",
        "      if pal == pas:\n",
        "        continue\n",
        "\n",
        "      #print(pal,'|',pas)\n",
        "      prefix = separate_left(pal,pas) # pal = prefix + pas [+suffix]\n",
        "      if len(prefix)>0 and prefix != pal and pal in past:\n",
        "        #print('prefix:',prefix)\n",
        "        if prefix.lower() in 'is, was, ; : ':  \n",
        "          past.remove(pal)\n",
        "          print('remove [x]:', pal)\n",
        "        elif prefix.lower() in 'if wish ':  \n",
        "          past.remove(pas)\n",
        "          print('remove [x]:', pas)\n",
        "\n",
        "      suffix=separate_right(pal,pas).strip()  # suffix = pal - prefix - pas \n",
        "      if len(suffix)>0 and suffix!=prefix: \n",
        "        #print('suffix:', suffix)\n",
        "        if suffix.lower().strip() in ', i , it, you , we , she , he , they , that , this , these , those , which , what , when , whom , who\\'s , and , why , as':\n",
        "          new = pal[0:pal.rfind(suffix)] # trim suffix\n",
        "          if pal in past:\n",
        "            past.remove(pal)\n",
        "            print('remove [x]:', pal, '[keep]:',new)             \n",
        "          if new not in past:\n",
        "            past.append(new) \n",
        "            print('replace[X]:',pal,'[with]:',new) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_cSjWSNI3oX"
      },
      "outputs": [],
      "source": [
        "def focus(spans):\n",
        "  for span1 in spans:\n",
        "    text1 =span1['text']\n",
        "    start1=span1['start']\n",
        "    end1 = span1['end']\n",
        "    if start1<0 or end1<0:\n",
        "      print(span1)\n",
        "\n",
        "    for span2 in spans:\n",
        "      text2 =span2['text']\n",
        "      start2=span2['start']\n",
        "      end2 = span2['end']\n",
        "      if start2<0 or end2<0:\n",
        "        print(span2)\n",
        "\n",
        "      if text1==text2:\n",
        "        continue\n",
        "\n",
        "      if start1 <= start2 and start2 < end1: \n",
        "        #print(start1,text1,end1,start2,text2,end2)\n",
        "\n",
        "        end = min(end1,end2)\n",
        "        if end1 > end and text1[end:end1].lower() in ', once , i , you , we , she , he , they , that , this , these , those , which , what , whom , who\\'s , when, and , why ':\n",
        "          if span1 in spans:\n",
        "            spans.remove(span1)\n",
        "        if end2 > end and text2[end:end2].lower() in ', once , i , you , we , she , he , they , that , this , these , those , which , what , whom , who\\'s , when, and , why ':\n",
        "          if span2 in spans:\n",
        "            spans.remove(span2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nietMZM3C-7"
      },
      "outputs": [],
      "source": [
        "def chunk(original):\n",
        "  verbs = list()\n",
        "  modal = list()\n",
        "  nounm = list()\n",
        "  past = list() \n",
        "  nounp = list()\n",
        "  nouns = list()\n",
        "\n",
        "  base = record(original)\n",
        "  original = original.replace('\\\"','')\n",
        "  removed = contract(original, base)\n",
        "  text = normalize(original) # remove extra spaces\n",
        "\n",
        "  prediction = predictor.predict(sentence=text)\n",
        "  mod, pas, nou, ver = extract_constituents(prediction)\n",
        " \n",
        "  for v in ver:\n",
        "    vr = simplify(v) \n",
        "    if vr not in verbs:\n",
        "      verbs.append(vr)\n",
        "\n",
        "  for u in nou:\n",
        "    nu = simplify(u) \n",
        "    if nu not in nouns:\n",
        "      nouns.append(nu) \n",
        "\n",
        "  if len(mod)==0: # if no modal('MD') detected \n",
        "    for p in pas:\n",
        "      pa = simplify(p)\n",
        "      if pa not in past:\n",
        "          past.append(pa) \n",
        "\n",
        "  for m in reversed(sorted(mod,key=len)):\n",
        "    md = simplify(m) # get one string gist\n",
        "    if len(md)==0:\n",
        "      continue\n",
        "    #print('md: '+md)\n",
        "    \n",
        "    for nm in reversed(sorted(nouns,key=len)):  \n",
        "      if nm in md:   \n",
        "        nounm.append(nm)\n",
        "        nouns.remove(nm)\n",
        "\n",
        "    for p in sorted(pas,key=len):\n",
        "      pa = simplify(p)\n",
        "      if len(pa)==0:\n",
        "        continue\n",
        "      #print('pa: ',pa)\n",
        "\n",
        "      for np in reversed(sorted(nouns,key=len)):\n",
        "        if np in pa:\n",
        "          nounp.append(np)\n",
        "          nouns.remove(np)\n",
        "\n",
        "      if pa not in past and len(pa) > 0:\n",
        "        past.append(pa)         \n",
        "\n",
        "    if md not in modal and len(md) > 0:        \n",
        "      modal.append(md)\n",
        "  \n",
        "  #print(past)\n",
        "  #print(modal)\n",
        "\n",
        "  deoverlap(past)\n",
        "\n",
        "  separate(modal, past)\n",
        "\n",
        "  #print(modal)\n",
        "  #print(past)\n",
        "\n",
        "  deoverlap(modal)\n",
        "  deoverlap(past)\n",
        "\n",
        "  #print(past)\n",
        "  #print(modal)\n",
        "\n",
        "  modals = defragment(modal, text, original, removed)\n",
        "  pasts = defragment(past, text, original, removed)\n",
        "\n",
        "  #if len(modals) > 1\n",
        "  #  focus(modals)\n",
        "  #if len(pasts) > 1\n",
        "  #  focus(pasts)\n",
        "  \n",
        "  #if len(pasts)==0: # because antecedent can't be empty\n",
        "  #  if len(modals) > 0:\n",
        "  #    pasts.append(modals.pop(0)) # why 0? can be any!\n",
        "  #    pasts[0].update(labels=1)   # gold\n",
        "  #    if len(modals) == 1:\n",
        "  #       modals[0].update(labels=1) # gold  \n",
        "  print('______________________________________________________________________________________________________________________________________________________________________________________________________________________________')\n",
        "  print(modals) \n",
        "  print(pasts)   \n",
        "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')  \n",
        "  \n",
        "  \n",
        "  return modals, pasts, verbs, nouns, nounm, nounp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quzZs7aWz7-l"
      },
      "outputs": [],
      "source": [
        "modals, pasts, verbs, nouns, nounm, nounp = chunk(\n",
        "  original = 'For example, people diagnosed with colon cancer might typically live beyond a decade, compared to just seven months if they had been diagnosed 40 years ago.'\n",
        ")   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "XRv9sHtMZQSE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "\n",
        "modal_df= pd.DataFrame(columns=['id','labels','text','start', 'end'])\n",
        "past_df = pd.DataFrame(columns=['id','labels','text','start', 'end'])\n",
        "simple_df = pd.DataFrame(columns=['id','labels','text'])\n",
        "\n",
        "for ind in tqdm(test_df.index): \n",
        "  original = test_df['text'][ind]\n",
        "  oid = test_df['id'][ind]\n",
        "\n",
        "  print(oid)\n",
        "  print(original)\n",
        "\n",
        "  modals, pasts, verbs, nouns, nounm, nounp = chunk(original)\n",
        "\n",
        "  i = 0\n",
        "  j = 0\n",
        "  for m in modals:\n",
        "    m_row = pd.Series(data={\n",
        "      'id': test_df['id'][ind]+'C'+ str(i),\n",
        "      'labels': m['labels'],\n",
        "      'text': m['text'] ,\n",
        "      'start': m['start'],\n",
        "      'end': m['end']\n",
        "    }, name = str(i) )\n",
        "    modal_df = modal_df.append(m_row)\n",
        "    i = i+1\n",
        " \n",
        "  for p in pasts:\n",
        "    p_row = pd.Series(data={\n",
        "      'id': test_df['id'][ind]+'A'+ str(j),\n",
        "      'labels': p['labels'],\n",
        "      'text': p['text'] ,\n",
        "      'start': p['start'],\n",
        "      'end': p['end']\n",
        "    }, name = str(j) )\n",
        "    past_df = past_df.append(p_row)   \n",
        "    j = j+1   \n",
        "\n",
        "  \n",
        "  sleep(1)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAQE-GxfQ4Ev"
      },
      "outputs": [],
      "source": [
        "past_df.to_csv(prefix+'antecedents.tsv', sep='\\t', index=False, header=False)\n",
        "modal_df.to_csv(prefix+'consequents.tsv', sep='\\t', index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3w26wdnuriO"
      },
      "outputs": [],
      "source": [
        "!mv /content/antecedents.tsv /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-2/\n",
        "!mv /content/consequents.tsv /content/gdrive/My\\ Drive/Colab\\ Notebooks/Subtask-2/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SemEval2020_Task5_Subtask_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
